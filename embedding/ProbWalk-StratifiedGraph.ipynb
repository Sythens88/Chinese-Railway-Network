{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prob Walk on Stratified Graph\n",
    "\n",
    "Stratified Walk: There are different types of railways in the graph (G,D,K,T,Z,C). So, each time, we only walk on one specific railway graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "np.random.seed(0)\n",
    "\n",
    "def merge(lists):\n",
    "    out = []\n",
    "    for l in lists:\n",
    "        out += l\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "railway_type = ['g','d','k','t','c','z','n']\n",
    "graph = {}\n",
    "for rail in railway_type:\n",
    "    graph[rail] = nx.read_edgelist('../graph/'+rail+'_undirected_graph.g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2826\n"
     ]
    }
   ],
   "source": [
    "nodes = list(set(merge([list(g.nodes) for g in graph.values()])))\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK_LENGTH = 20\n",
    "WALK_PER_VERTEX = 20\n",
    "WINDOW_SIZE = 2\n",
    "K = 5\n",
    "BATCH_SIZE = 128\n",
    "EMBED_DIM = 8\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "LEARNING_RATE = 0.2\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2idx = {node:i for i,node in enumerate(nodes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_freq = {}\n",
    "for rail in railway_type:\n",
    "    count = []\n",
    "    for node in nodes:\n",
    "        if node in graph[rail].nodes:\n",
    "            count.append(sum([value['weight'] for value in graph[rail][node].values()]))\n",
    "        else:\n",
    "            count.append(0)\n",
    "    count = np.array(count)\n",
    "    freq = count/np.sum(count)\n",
    "    node_freq[rail] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StratifiedProbWalk(graph,node,walk_length):\n",
    "    choice_graph ={key:sum([value['weight'] for value in graph[key][node].values()])\\\n",
    "         for key, value in graph.items() if node in value.nodes}\n",
    "    prob = np.array(list(choice_graph.values()))\n",
    "    prob = prob/np.sum(prob)\n",
    "    layer = np.random.choice(list(choice_graph.keys()),1,p=prob)[0]\n",
    "    graph = graph[layer]\n",
    "    \n",
    "    path = [node]\n",
    "    for _ in range(walk_length):\n",
    "        neighbour = graph[node]\n",
    "        prob = np.array([value['weight'] for value in neighbour.values()])\n",
    "        prob = prob/np.sum(prob)\n",
    "        node_next = np.random.choice(list(neighbour.keys()),1,p=prob)\n",
    "        path.append(node_next[0])\n",
    "        node = node_next[0]\n",
    "        \n",
    "    return path, layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56520\n",
      "Counter({'k': 19068, 'n': 17509, 'g': 6865, 'd': 4222, 't': 4143, 'c': 3852, 'z': 861})\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "rail_type = []\n",
    "for _ in range(WALK_PER_VERTEX):\n",
    "    for node in nodes:\n",
    "        path, layer = StratifiedProbWalk(graph,node,WALK_LENGTH)\n",
    "        corpus.append(path)\n",
    "        rail_type.append(layer)\n",
    "\n",
    "print(len(corpus))\n",
    "print(Counter(rail_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pairs = []\n",
    "for path in corpus:\n",
    "    for i in range(len(path)):\n",
    "        idxs = (list(range(i-WINDOW_SIZE, i)) + list(range(i+1, i+WINDOW_SIZE+1)))\n",
    "        idxs = [idx for idx in idxs if idx>=0 and idx<=WALK_LENGTH]\n",
    "        if len(idxs)==2*WINDOW_SIZE:\n",
    "            pos_pairs += [ [path[i],[path[idx] for idx in idxs] ] ]\n",
    "\n",
    "rail_type = merge([[t]*(WALK_LENGTH-WINDOW_SIZE-1) for t in rail_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960840 960840\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_pairs),len(rail_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(tud.Dataset):\n",
    "    def __init__(self,pos_pairs,rail_type,node2idx,node_freq,K):\n",
    "        super(GraphDataset,self).__init__()\n",
    "        \n",
    "        self.center_node = [ node2idx[pair[0]] for pair in pos_pairs] \n",
    "        self.pos_pairs = [[node2idx[p] for p in pair[1]] for pair in pos_pairs]\n",
    "        self.center_node = torch.Tensor(self.center_node).long()\n",
    "        self.pos_pairs = torch.Tensor(self.pos_pairs).long()\n",
    "        self.rail_type = rail_type\n",
    "        self.node_freq = {key:torch.Tensor(value) for key,value in node_freq.items()}\n",
    "        self.K = K\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.center_node)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        center_node = self.center_node[idx]\n",
    "        pos_nodes = self.pos_pairs[idx]\n",
    "        rail = self.rail_type[idx]\n",
    "        neg_nodes = torch.multinomial(self.node_freq[rail], self.K * pos_nodes.shape[0], True)\n",
    "        \n",
    "        return center_node, pos_nodes, neg_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphDataset(pos_pairs,rail_type,node2idx,node_freq,K)\n",
    "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self,node_size,embed_dim):\n",
    "        super(NodeEmbedding,self).__init__()\n",
    "        self.node_size = node_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.in_embed = nn.Embedding(node_size,embed_dim)\n",
    "        self.out_embed = nn.Embedding(node_size,embed_dim)\n",
    "        \n",
    "        initrange = 0.5/embed_dim\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, center_node, pos_nodes, neg_nodes):\n",
    "        \n",
    "        center_emb = self.in_embed(center_node)   # bs*emb_dim\n",
    "        pos_emb = self.out_embed(pos_nodes) # bs*(2*ws)*emb_dim\n",
    "        neg_emb = self.out_embed(neg_nodes) # bs*(2*ws*K)*emb_dim\n",
    "        \n",
    "        loss_pos = torch.bmm(pos_emb, center_emb.unsqueeze(2)).squeeze()  # bs*(2*ws)\n",
    "        loss_neg = torch.bmm(neg_emb, -center_emb.unsqueeze(2)).squeeze() # bs*(2*ws*K)\n",
    "\n",
    "        loss_pos = F.logsigmoid(loss_pos).sum(1)\n",
    "        loss_neg = F.logsigmoid(loss_neg).sum(1) # batch_size\n",
    "       \n",
    "        loss = loss_pos + loss_neg\n",
    "        \n",
    "        return -loss\n",
    "    \n",
    "    def get_embed(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy().tolist()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NodeEmbedding(len(nodes),EMBED_DIM)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: 16.635223388671875\n",
      "epoch: 0, iter: 2000, loss: 16.616764068603516\n",
      "epoch: 0, iter: 4000, loss: 14.976537704467773\n",
      "epoch: 0, iter: 6000, loss: 11.731965065002441\n",
      "epoch: 1, iter: 0, loss: 10.44923210144043\n",
      "epoch: 1, iter: 2000, loss: 9.22749137878418\n",
      "epoch: 1, iter: 4000, loss: 8.145805358886719\n",
      "epoch: 1, iter: 6000, loss: 7.470710754394531\n",
      "epoch: 2, iter: 0, loss: 6.97938346862793\n",
      "epoch: 2, iter: 2000, loss: 6.119506359100342\n",
      "epoch: 2, iter: 4000, loss: 5.633045196533203\n",
      "epoch: 2, iter: 6000, loss: 4.989376068115234\n",
      "epoch: 3, iter: 0, loss: 4.6959547996521\n",
      "epoch: 3, iter: 2000, loss: 4.5140838623046875\n",
      "epoch: 3, iter: 4000, loss: 3.805485248565674\n",
      "epoch: 3, iter: 6000, loss: 3.6393494606018066\n",
      "epoch: 4, iter: 0, loss: 3.7159042358398438\n",
      "epoch: 4, iter: 2000, loss: 3.2905123233795166\n",
      "epoch: 4, iter: 4000, loss: 2.970101833343506\n",
      "epoch: 4, iter: 6000, loss: 3.0207319259643555\n",
      "epoch: 5, iter: 0, loss: 2.8857178688049316\n",
      "epoch: 5, iter: 2000, loss: 2.743900775909424\n",
      "epoch: 5, iter: 4000, loss: 2.6207337379455566\n",
      "epoch: 5, iter: 6000, loss: 3.0521278381347656\n",
      "epoch: 6, iter: 0, loss: 2.3559985160827637\n",
      "epoch: 6, iter: 2000, loss: 2.5367512702941895\n",
      "epoch: 6, iter: 4000, loss: 2.2859628200531006\n",
      "epoch: 6, iter: 6000, loss: 2.205382823944092\n",
      "epoch: 7, iter: 0, loss: 2.544006109237671\n",
      "epoch: 7, iter: 2000, loss: 2.085610866546631\n",
      "epoch: 7, iter: 4000, loss: 2.227203607559204\n",
      "epoch: 7, iter: 6000, loss: 2.376129388809204\n",
      "epoch: 8, iter: 0, loss: 2.2337918281555176\n",
      "epoch: 8, iter: 2000, loss: 2.386413097381592\n",
      "epoch: 8, iter: 4000, loss: 2.204813241958618\n",
      "epoch: 8, iter: 6000, loss: 1.9651414155960083\n",
      "epoch: 9, iter: 0, loss: 2.285883903503418\n",
      "epoch: 9, iter: 2000, loss: 1.9552693367004395\n",
      "epoch: 9, iter: 4000, loss: 1.776653528213501\n",
      "epoch: 9, iter: 6000, loss: 2.0335986614227295\n",
      "epoch: 10, iter: 0, loss: 2.1915817260742188\n",
      "epoch: 10, iter: 2000, loss: 1.8704559803009033\n",
      "epoch: 10, iter: 4000, loss: 2.173811912536621\n",
      "epoch: 10, iter: 6000, loss: 1.7527363300323486\n",
      "epoch: 11, iter: 0, loss: 2.120269298553467\n",
      "epoch: 11, iter: 2000, loss: 2.1488263607025146\n",
      "epoch: 11, iter: 4000, loss: 1.772080898284912\n",
      "epoch: 11, iter: 6000, loss: 1.907079815864563\n",
      "epoch: 12, iter: 0, loss: 2.2057852745056152\n",
      "epoch: 12, iter: 2000, loss: 1.6839672327041626\n",
      "epoch: 12, iter: 4000, loss: 1.8836355209350586\n",
      "epoch: 12, iter: 6000, loss: 1.9179997444152832\n",
      "epoch: 13, iter: 0, loss: 1.7049238681793213\n",
      "epoch: 13, iter: 2000, loss: 1.7403967380523682\n",
      "epoch: 13, iter: 4000, loss: 1.7636680603027344\n",
      "epoch: 13, iter: 6000, loss: 1.7637686729431152\n",
      "epoch: 14, iter: 0, loss: 1.9032646417617798\n",
      "epoch: 14, iter: 2000, loss: 1.906238079071045\n",
      "epoch: 14, iter: 4000, loss: 1.9992314577102661\n",
      "epoch: 14, iter: 6000, loss: 1.9665019512176514\n",
      "epoch: 15, iter: 0, loss: 1.8048783540725708\n",
      "epoch: 15, iter: 2000, loss: 1.7088685035705566\n",
      "epoch: 15, iter: 4000, loss: 1.7081284523010254\n",
      "epoch: 15, iter: 6000, loss: 1.7236219644546509\n",
      "epoch: 16, iter: 0, loss: 1.8470630645751953\n",
      "epoch: 16, iter: 2000, loss: 2.1355209350585938\n",
      "epoch: 16, iter: 4000, loss: 1.5969208478927612\n",
      "epoch: 16, iter: 6000, loss: 1.8419965505599976\n",
      "epoch: 17, iter: 0, loss: 2.0234665870666504\n",
      "epoch: 17, iter: 2000, loss: 2.1329808235168457\n",
      "epoch: 17, iter: 4000, loss: 1.8705307245254517\n",
      "epoch: 17, iter: 6000, loss: 1.697322964668274\n",
      "epoch: 18, iter: 0, loss: 1.4800938367843628\n",
      "epoch: 18, iter: 2000, loss: 1.5010379552841187\n",
      "epoch: 18, iter: 4000, loss: 1.5991795063018799\n",
      "epoch: 18, iter: 6000, loss: 1.9809587001800537\n",
      "epoch: 19, iter: 0, loss: 1.6983246803283691\n",
      "epoch: 19, iter: 2000, loss: 1.35408616065979\n",
      "epoch: 19, iter: 4000, loss: 1.575322151184082\n",
      "epoch: 19, iter: 6000, loss: 1.35613214969635\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        center_node, pos_nodes, neg_nodes = map(lambda x:x.long().to(DEVICE), batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(center_node, pos_nodes, neg_nodes).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if i % 2000 == 0:\n",
    "            print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict = dict(zip(node2idx.keys(),model.get_embed()))\n",
    "pd.DataFrame(embed_dict).T.to_csv('embedding/prob_walk_stratified_graph_'+str(EMBED_DIM)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('loss/prob_walk_stratified_graph_'+str(EMBED_DIM)+'.npy',losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
