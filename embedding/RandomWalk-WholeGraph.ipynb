{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walk on Whole Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.read_edgelist('../graph/whole_undirected_graph.g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2826\n"
     ]
    }
   ],
   "source": [
    "nodes = list(graph.nodes)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK_LENGTH = 20\n",
    "WALK_PER_VERTEX = 20\n",
    "WINDOW_SIZE = 2\n",
    "K = 5\n",
    "BATCH_SIZE = 128\n",
    "EMBED_DIM = 8\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "LEARNING_RATE = 0.2\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2idx = {node:i for i,node in enumerate(nodes)}\n",
    "node_count = np.array([graph.degree[node] for node in nodes])\n",
    "node_freq = node_count/np.sum(node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomWalk(graph,start_node,walk_length):\n",
    "    path = [start_node]\n",
    "    node = start_node\n",
    "    for _ in range(walk_length):\n",
    "        node_next = np.random.choice(graph[node])\n",
    "        path.append(node_next)\n",
    "        node = node_next\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56520\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for _ in range(WALK_PER_VERTEX):\n",
    "    for node in graph.nodes:\n",
    "        corpus.append(RandomWalk(graph,node,WALK_LENGTH))\n",
    "\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_pairs = []\n",
    "for path in corpus:\n",
    "    for i in range(len(path)):\n",
    "        idxs = (list(range(i-WINDOW_SIZE, i)) + list(range(i+1, i+WINDOW_SIZE+1)))\n",
    "        idxs = [idx for idx in idxs if idx>=0 and idx<=WALK_LENGTH]\n",
    "        if len(idxs)==2*WINDOW_SIZE:\n",
    "            pos_pairs += [ [path[i],[path[idx] for idx in idxs] ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(tud.Dataset):\n",
    "    def __init__(self,pos_pairs,node2idx,node_freq,K):\n",
    "        super(GraphDataset,self).__init__()\n",
    "        \n",
    "        self.center_node = [ node2idx[pair[0]] for pair in pos_pairs] \n",
    "        self.pos_pairs = [[node2idx[p] for p in pair[1]] for pair in pos_pairs]\n",
    "        self.center_node = torch.Tensor(self.center_node).long()\n",
    "        self.pos_pairs = torch.Tensor(self.pos_pairs).long()\n",
    "        self.node_freq = torch.Tensor(node_freq)\n",
    "        self.K = K\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.center_node)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        center_node = self.center_node[idx]\n",
    "        pos_nodes = self.pos_pairs[idx]\n",
    "        neg_nodes = torch.multinomial(self.node_freq, self.K * pos_nodes.shape[0], True)\n",
    "        \n",
    "        return center_node, pos_nodes, neg_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphDataset(pos_pairs,node2idx,node_freq,K)\n",
    "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self,node_size,embed_dim):\n",
    "        super(NodeEmbedding,self).__init__()\n",
    "        self.node_size = node_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.in_embed = nn.Embedding(node_size,embed_dim)\n",
    "        self.out_embed = nn.Embedding(node_size,embed_dim)\n",
    "        \n",
    "        initrange = 0.5/embed_dim\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, center_node, pos_nodes, neg_nodes):\n",
    "        \n",
    "        center_emb = self.in_embed(center_node)   # bs*emb_dim\n",
    "        pos_emb = self.out_embed(pos_nodes) # bs*(2*ws)*emb_dim\n",
    "        neg_emb = self.out_embed(neg_nodes) # bs*(2*ws*K)*emb_dim\n",
    "        \n",
    "        loss_pos = torch.bmm(pos_emb, center_emb.unsqueeze(2)).squeeze()  # bs*(2*ws)\n",
    "        loss_neg = torch.bmm(neg_emb, -center_emb.unsqueeze(2)).squeeze() # bs*(2*ws*K)\n",
    "\n",
    "        loss_pos = F.logsigmoid(loss_pos).sum(1)\n",
    "        loss_neg = F.logsigmoid(loss_neg).sum(1) # batch_size\n",
    "       \n",
    "        loss = loss_pos + loss_neg\n",
    "        \n",
    "        return -loss\n",
    "    \n",
    "    def get_embed(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy().tolist()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NodeEmbedding(len(graph.nodes),EMBED_DIM)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: 16.635639190673828\n",
      "epoch: 0, iter: 2000, loss: 16.632469177246094\n",
      "epoch: 0, iter: 4000, loss: 16.53997039794922\n",
      "epoch: 0, iter: 6000, loss: 14.52586555480957\n",
      "epoch: 1, iter: 0, loss: 12.094035148620605\n",
      "epoch: 1, iter: 2000, loss: 10.832967758178711\n",
      "epoch: 1, iter: 4000, loss: 10.425745010375977\n",
      "epoch: 1, iter: 6000, loss: 9.422791481018066\n",
      "epoch: 2, iter: 0, loss: 8.974201202392578\n",
      "epoch: 2, iter: 2000, loss: 7.940958023071289\n",
      "epoch: 2, iter: 4000, loss: 7.051218509674072\n",
      "epoch: 2, iter: 6000, loss: 6.262929916381836\n",
      "epoch: 3, iter: 0, loss: 5.802253246307373\n",
      "epoch: 3, iter: 2000, loss: 4.948734283447266\n",
      "epoch: 3, iter: 4000, loss: 4.57375431060791\n",
      "epoch: 3, iter: 6000, loss: 4.370632171630859\n",
      "epoch: 4, iter: 0, loss: 3.911592960357666\n",
      "epoch: 4, iter: 2000, loss: 3.665191888809204\n",
      "epoch: 4, iter: 4000, loss: 3.224729537963867\n",
      "epoch: 4, iter: 6000, loss: 3.1978776454925537\n",
      "epoch: 5, iter: 0, loss: 3.340299129486084\n",
      "epoch: 5, iter: 2000, loss: 3.0534088611602783\n",
      "epoch: 5, iter: 4000, loss: 2.8226265907287598\n",
      "epoch: 5, iter: 6000, loss: 2.6733951568603516\n",
      "epoch: 6, iter: 0, loss: 2.5033130645751953\n",
      "epoch: 6, iter: 2000, loss: 2.3187336921691895\n",
      "epoch: 6, iter: 4000, loss: 2.4525041580200195\n",
      "epoch: 6, iter: 6000, loss: 2.217888355255127\n",
      "epoch: 7, iter: 0, loss: 2.3920974731445312\n",
      "epoch: 7, iter: 2000, loss: 2.0991272926330566\n",
      "epoch: 7, iter: 4000, loss: 2.433363437652588\n",
      "epoch: 7, iter: 6000, loss: 2.1449146270751953\n",
      "epoch: 8, iter: 0, loss: 2.029390335083008\n",
      "epoch: 8, iter: 2000, loss: 2.117861270904541\n",
      "epoch: 8, iter: 4000, loss: 2.19791841506958\n",
      "epoch: 8, iter: 6000, loss: 2.0185818672180176\n",
      "epoch: 9, iter: 0, loss: 1.9252606630325317\n",
      "epoch: 9, iter: 2000, loss: 1.8870737552642822\n",
      "epoch: 9, iter: 4000, loss: 1.9602290391921997\n",
      "epoch: 9, iter: 6000, loss: 1.8141289949417114\n",
      "epoch: 10, iter: 0, loss: 2.1998071670532227\n",
      "epoch: 10, iter: 2000, loss: 1.5945485830307007\n",
      "epoch: 10, iter: 4000, loss: 1.5955047607421875\n",
      "epoch: 10, iter: 6000, loss: 1.6747634410858154\n",
      "epoch: 11, iter: 0, loss: 1.9369285106658936\n",
      "epoch: 11, iter: 2000, loss: 2.0117502212524414\n",
      "epoch: 11, iter: 4000, loss: 1.9335500001907349\n",
      "epoch: 11, iter: 6000, loss: 1.6023483276367188\n",
      "epoch: 12, iter: 0, loss: 1.792487621307373\n",
      "epoch: 12, iter: 2000, loss: 1.7414048910140991\n",
      "epoch: 12, iter: 4000, loss: 1.8262100219726562\n",
      "epoch: 12, iter: 6000, loss: 1.4984488487243652\n",
      "epoch: 13, iter: 0, loss: 1.5856883525848389\n",
      "epoch: 13, iter: 2000, loss: 2.041656970977783\n",
      "epoch: 13, iter: 4000, loss: 1.6003148555755615\n",
      "epoch: 13, iter: 6000, loss: 1.9165936708450317\n",
      "epoch: 14, iter: 0, loss: 1.5234496593475342\n",
      "epoch: 14, iter: 2000, loss: 1.6553796529769897\n",
      "epoch: 14, iter: 4000, loss: 1.5695741176605225\n",
      "epoch: 14, iter: 6000, loss: 1.5416775941848755\n",
      "epoch: 15, iter: 0, loss: 1.5493931770324707\n",
      "epoch: 15, iter: 2000, loss: 1.5676870346069336\n",
      "epoch: 15, iter: 4000, loss: 1.580885410308838\n",
      "epoch: 15, iter: 6000, loss: 1.8565864562988281\n",
      "epoch: 16, iter: 0, loss: 1.5579209327697754\n",
      "epoch: 16, iter: 2000, loss: 1.737433671951294\n",
      "epoch: 16, iter: 4000, loss: 1.6578344106674194\n",
      "epoch: 16, iter: 6000, loss: 1.7001625299453735\n",
      "epoch: 17, iter: 0, loss: 1.3382244110107422\n",
      "epoch: 17, iter: 2000, loss: 1.8591742515563965\n",
      "epoch: 17, iter: 4000, loss: 1.8409839868545532\n",
      "epoch: 17, iter: 6000, loss: 1.718648910522461\n",
      "epoch: 18, iter: 0, loss: 1.6165919303894043\n",
      "epoch: 18, iter: 2000, loss: 1.605072259902954\n",
      "epoch: 18, iter: 4000, loss: 1.711787462234497\n",
      "epoch: 18, iter: 6000, loss: 1.4777635335922241\n",
      "epoch: 19, iter: 0, loss: 1.4365339279174805\n",
      "epoch: 19, iter: 2000, loss: 1.5704712867736816\n",
      "epoch: 19, iter: 4000, loss: 1.3912858963012695\n",
      "epoch: 19, iter: 6000, loss: 1.4816210269927979\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        center_node, pos_nodes, neg_nodes = map(lambda x:x.long().to(DEVICE), batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(center_node, pos_nodes, neg_nodes).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if i % 2000 == 0:\n",
    "            print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict = dict(zip(node2idx.keys(),model.get_embed()))\n",
    "pd.DataFrame(embed_dict).T.to_csv('embedding/random_walk_whole_graph_'+str(EMBED_DIM)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('loss/random_walk_whole_graph_'+str(EMBED_DIM)+'.npy',losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
